{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci_dir = 'clear_songci_data.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci = []\n",
    "\n",
    "with open(ci_dir, 'r', encoding = 'utf-8') as text:\n",
    "    for line in text:\n",
    "        ci.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21042"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_word = []\n",
    "\n",
    "for i in range(len(ci)):\n",
    "    for line in ci[i]:\n",
    "        for word in line:\n",
    "            all_word.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_word_dict = sorted(set(all_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5328"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_word 是所有诗词单个字的组合\n",
    "# all_word_dict 是所有出现的字\n",
    "\n",
    "word_to_int = {word: i for i, word in enumerate(all_word_dict)}\n",
    "int_to_word = dict(enumerate(all_word_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = np.array([word_to_int[i] for i in all_word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(input_data, num_sequence, steps):\n",
    "    \n",
    "    # num_sequence: 每批数据分成多少个sequence\n",
    "    # char_per_batch: 每批有多少个字\n",
    "    # steps: 每批次要多少步\n",
    "    char_per_batch = num_sequence * steps\n",
    "    \n",
    "    # 数据分成多少批\n",
    "    num_batches = len(input_data) // char_per_batch\n",
    "    \n",
    "    # 避免reshape的时候出错\n",
    "    input_data = input_data[ :char_per_batch * num_batches]\n",
    "    \n",
    "    '''\n",
    "    num_sequence * steps * num_batches = input_data\n",
    "    把input_data变成shape( num_sequence, steps * num_batches)\n",
    "    在只generate一次的情况下，shape = (num_sequence, steps)\n",
    "    '''\n",
    "    input_data = np.reshape(input_data, (num_sequence, -1))\n",
    "    \n",
    "    # (0, steps * num_batches, steps) = (0, input_data.shape[1], steps)\n",
    "    # num = num_batches\n",
    "    for i in range(0, num_batches * steps, steps):\n",
    "        x = input_data[ : , i:i + steps]\n",
    "        y_temp = input_data[ : , i+1: i+steps+1]\n",
    "        \n",
    "        y = np.zeros(x.shape, dtype = x.dtype)\n",
    "        y[:, :y_temp.shape[1]] = y_temp\n",
    "        \n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1583790"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 两万首词；分成100个batch；每次训练50步；每个batch的字数是100*50 = 5000；\n",
    "# 20000 / 5000 = 4; 也就是分成4批数据\n",
    "len(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个函数其实是一个生成函数\n",
    "# 在这里如果num_sequence = 100, steps = 100，那么二者相乘就是10000\n",
    "# 这样的话这个生成函数就会生成150次(总数为150万字)\n",
    "\n",
    "test_x, test_y = next(next_batch(encoded, num_sequence = 100, steps = 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3123 2396 1781   77 1121   42 2017 3153 2036 2373]\n",
      " [4102 2450 2595 2062   42 2247 3221  478 2336 5314]\n",
      " [5314 3766 1134 3971   42  595 3847 3379 2091 1342]]\n",
      "\n",
      "[[2396 1781   77 1121   42 2017 3153 2036 2373 1141]\n",
      " [2450 2595 2062   42 2247 3221  478 2336 5314 4177]\n",
      " [3766 1134 3971   42  595 3847 3379 2091 1342 5314]]\n"
     ]
    }
   ],
   "source": [
    "print(test_x[:3, :10])\n",
    "print('')\n",
    "print(test_y[:3, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_placeholder(sequence_size, steps):\n",
    "    x = tf.placeholder(dtype = tf.int32, shape = [sequence_size, steps], name = 'input_x')\n",
    "    y = tf.placeholder(dtype = tf.int32, shape = [sequence_size, steps], name = 'target_y')\n",
    "    keep_prob = tf.placeholder(dtype = tf.float32, name = 'keep_prob')\n",
    "    return x, y, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_cell(num_units, keep_prob, sequence_size, num_layers):\n",
    "    \n",
    "    '''\n",
    "    返回cell和initial_state\n",
    "    '''\n",
    "    \n",
    "    # 先用一个函数生成 cell\n",
    "    def build_cell(num_units, keep_prob):\n",
    "        cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "        drop = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob = keep_prob)\n",
    "        return drop\n",
    "    \n",
    "    # 有多少个layers，就有多少个cell\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([build_cell(num_units, keep_prob) for _ in range(num_layers)])\n",
    "    initial_state = cell.zero_state(sequence_size, tf.float32)\n",
    "    \n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_output(cell, input_size, output_size):\n",
    "    \n",
    "    # 把各个cell整合在一起\n",
    "    rnn_cell = tf.concat(cell, axis = 1)\n",
    "    rnn_cell = tf.reshape(rnn_cell, shape = (-1, input_size))\n",
    "    \n",
    "    with tf.variable_scope('softmax'): \n",
    "        \n",
    "        # input_size = lstm_size; \n",
    "        # output_size = num_classes\n",
    "        \n",
    "        softmax_w = tf.Variable(tf.truncated_normal(shape = [input_size, output_size], stddev = 0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(shape = [output_size]))\n",
    "        \n",
    "    logits = tf.matmul(rnn_cell, softmax_w) + softmax_b\n",
    "    outputs = tf.nn.softmax(logits)\n",
    "    \n",
    "    return outputs, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(logits, targets, lstm_size, num_classes ):\n",
    "    \n",
    "    y_ont_hot = tf.one_hot(targets, num_classes)\n",
    "    # logits_shape = (-1, num_classes)\n",
    "    y_reshape = tf.reshape(y_ont_hot, logits.get_shape())\n",
    "    \n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels = y_reshape)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(loss, grad_clip, learning_rate):\n",
    "    \n",
    "    # 返回所有trainable的变量\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train = optimizer.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN():\n",
    "    \n",
    "    def __init__(self, learning_rate, num_classes, sequence_size = 64, steps = 100,\n",
    "                 num_units = 256, num_layers = 2, grad_clip = 5,sampling = False):\n",
    "        \n",
    "        if sampling == True:\n",
    "            sequence_size, steps = 1, 1\n",
    "        else:\n",
    "            sequence_size, steps = sequence_size, steps\n",
    "            \n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # 输入数据\n",
    "        self.x_input, self.y_target, self.keep_prob = create_placeholder(sequence_size, steps)        \n",
    "        cell, self.initial_state = create_lstm_cell(num_units, keep_prob, sequence_size, num_layers)\n",
    "        \n",
    "        # ont_hot\n",
    "        x_one_hot = tf.one_hot(self.x_input, depth = num_classes)\n",
    "        \n",
    "        # 构建rnn结构\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state = self.initial_state)\n",
    "        self.final_state = state\n",
    "        \n",
    "        # 输出数据\n",
    "        self.prediction, self.logits = create_output(cell = outputs, input_size = num_units, output_size = num_classes)\n",
    "        \n",
    "        # loss function\n",
    "        self.loss = loss(self.logits, self.y_target, num_units, num_classes)\n",
    "        self.optimizer = build_optimizer(self.loss, grad_clip, learning_rate)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_size = 64\n",
    "steps = 80\n",
    "num_units = 512\n",
    "num_layers = 4\n",
    "learning_rate = 0.0023\n",
    "keep_prob = 0.9\n",
    "num_classes = len(word_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharRNN(learning_rate, num_classes, sequence_size = sequence_size, steps = steps, num_units = num_units,\n",
    "               num_layers = num_layers, grad_clip = 5, sampling = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20\n",
      " Training Steps: 64...\n",
      " Training loss: 6.4185\n",
      " 0.7593 sec/batch\n",
      "\n",
      "Epoch: 1/20\n",
      " Training Steps: 128...\n",
      " Training loss: 6.1022\n",
      " 0.7610 sec/batch\n",
      "\n",
      "Epoch: 1/20\n",
      " Training Steps: 192...\n",
      " Training loss: 6.0954\n",
      " 0.7589 sec/batch\n",
      "\n",
      "Epoch: 1/20\n",
      " Training Steps: 256...\n",
      " Training loss: 5.9429\n",
      " 0.7584 sec/batch\n",
      "\n",
      "Epoch: 2/20\n",
      " Training Steps: 320...\n",
      " Training loss: 5.7035\n",
      " 0.7616 sec/batch\n",
      "\n",
      "Epoch: 2/20\n",
      " Training Steps: 384...\n",
      " Training loss: 5.5873\n",
      " 0.7613 sec/batch\n",
      "\n",
      "Epoch: 2/20\n",
      " Training Steps: 448...\n",
      " Training loss: 5.5276\n",
      " 0.7610 sec/batch\n",
      "\n",
      "Epoch: 2/20\n",
      " Training Steps: 512...\n",
      " Training loss: 5.3831\n",
      " 0.7627 sec/batch\n",
      "\n",
      "Epoch: 2/20\n",
      " Training Steps: 576...\n",
      " Training loss: 5.2447\n",
      " 0.7623 sec/batch\n",
      "\n",
      "Epoch: 3/20\n",
      " Training Steps: 640...\n",
      " Training loss: 5.1006\n",
      " 0.7624 sec/batch\n",
      "\n",
      "Epoch: 3/20\n",
      " Training Steps: 704...\n",
      " Training loss: 5.0606\n",
      " 0.7676 sec/batch\n",
      "\n",
      "Epoch: 3/20\n",
      " Training Steps: 768...\n",
      " Training loss: 5.0820\n",
      " 0.7667 sec/batch\n",
      "\n",
      "Epoch: 3/20\n",
      " Training Steps: 832...\n",
      " Training loss: 4.9792\n",
      " 0.7632 sec/batch\n",
      "\n",
      "Epoch: 3/20\n",
      " Training Steps: 896...\n",
      " Training loss: 5.0013\n",
      " 0.7617 sec/batch\n",
      "\n",
      "Epoch: 4/20\n",
      " Training Steps: 960...\n",
      " Training loss: 4.8499\n",
      " 0.7637 sec/batch\n",
      "\n",
      "Epoch: 4/20\n",
      " Training Steps: 1024...\n",
      " Training loss: 4.7747\n",
      " 0.7621 sec/batch\n",
      "\n",
      "Epoch: 4/20\n",
      " Training Steps: 1088...\n",
      " Training loss: 4.7161\n",
      " 0.7596 sec/batch\n",
      "\n",
      "Epoch: 4/20\n",
      " Training Steps: 1152...\n",
      " Training loss: 4.7740\n",
      " 0.7602 sec/batch\n",
      "\n",
      "Epoch: 4/20\n",
      " Training Steps: 1216...\n",
      " Training loss: 4.6129\n",
      " 0.7592 sec/batch\n",
      "\n",
      "Epoch: 5/20\n",
      " Training Steps: 1280...\n",
      " Training loss: 4.5787\n",
      " 0.7634 sec/batch\n",
      "\n",
      "Epoch: 5/20\n",
      " Training Steps: 1344...\n",
      " Training loss: 4.5782\n",
      " 0.7601 sec/batch\n",
      "\n",
      "Epoch: 5/20\n",
      " Training Steps: 1408...\n",
      " Training loss: 4.5363\n",
      " 0.7615 sec/batch\n",
      "\n",
      "Epoch: 5/20\n",
      " Training Steps: 1472...\n",
      " Training loss: 4.6015\n",
      " 0.7611 sec/batch\n",
      "\n",
      "Epoch: 5/20\n",
      " Training Steps: 1536...\n",
      " Training loss: 4.3991\n",
      " 0.7645 sec/batch\n",
      "\n",
      "Epoch: 6/20\n",
      " Training Steps: 1600...\n",
      " Training loss: 4.3760\n",
      " 0.7614 sec/batch\n",
      "\n",
      "Epoch: 6/20\n",
      " Training Steps: 1664...\n",
      " Training loss: 4.5050\n",
      " 0.7606 sec/batch\n",
      "\n",
      "Epoch: 6/20\n",
      " Training Steps: 1728...\n",
      " Training loss: 4.4514\n",
      " 0.7635 sec/batch\n",
      "\n",
      "Epoch: 6/20\n",
      " Training Steps: 1792...\n",
      " Training loss: 4.3595\n",
      " 0.7612 sec/batch\n",
      "\n",
      "Epoch: 7/20\n",
      " Training Steps: 1856...\n",
      " Training loss: 4.3100\n",
      " 0.7591 sec/batch\n",
      "\n",
      "Epoch: 7/20\n",
      " Training Steps: 1920...\n",
      " Training loss: 4.2674\n",
      " 0.7613 sec/batch\n",
      "\n",
      "Epoch: 7/20\n",
      " Training Steps: 1984...\n",
      " Training loss: 4.3259\n",
      " 0.7590 sec/batch\n",
      "\n",
      "Epoch: 7/20\n",
      " Training Steps: 2048...\n",
      " Training loss: 4.2217\n",
      " 0.7604 sec/batch\n",
      "\n",
      "Epoch: 7/20\n",
      " Training Steps: 2112...\n",
      " Training loss: 4.1769\n",
      " 0.7628 sec/batch\n",
      "\n",
      "Epoch: 8/20\n",
      " Training Steps: 2176...\n",
      " Training loss: 4.2344\n",
      " 0.7594 sec/batch\n",
      "\n",
      "Epoch: 8/20\n",
      " Training Steps: 2240...\n",
      " Training loss: 4.1199\n",
      " 0.7616 sec/batch\n",
      "\n",
      "Epoch: 8/20\n",
      " Training Steps: 2304...\n",
      " Training loss: 4.1810\n",
      " 0.7627 sec/batch\n",
      "\n",
      "Epoch: 8/20\n",
      " Training Steps: 2368...\n",
      " Training loss: 4.2061\n",
      " 0.7708 sec/batch\n",
      "\n",
      "Epoch: 8/20\n",
      " Training Steps: 2432...\n",
      " Training loss: 4.1292\n",
      " 0.7592 sec/batch\n",
      "\n",
      "Epoch: 9/20\n",
      " Training Steps: 2496...\n",
      " Training loss: 4.1242\n",
      " 0.7605 sec/batch\n",
      "\n",
      "Epoch: 9/20\n",
      " Training Steps: 2560...\n",
      " Training loss: 4.0266\n",
      " 0.7596 sec/batch\n",
      "\n",
      "Epoch: 9/20\n",
      " Training Steps: 2624...\n",
      " Training loss: 4.1187\n",
      " 0.7605 sec/batch\n",
      "\n",
      "Epoch: 9/20\n",
      " Training Steps: 2688...\n",
      " Training loss: 4.1197\n",
      " 0.7623 sec/batch\n",
      "\n",
      "Epoch: 9/20\n",
      " Training Steps: 2752...\n",
      " Training loss: 4.0955\n",
      " 0.7616 sec/batch\n",
      "\n",
      "Epoch: 10/20\n",
      " Training Steps: 2816...\n",
      " Training loss: 4.0188\n",
      " 0.7608 sec/batch\n",
      "\n",
      "Epoch: 10/20\n",
      " Training Steps: 2880...\n",
      " Training loss: 4.0449\n",
      " 0.7621 sec/batch\n",
      "\n",
      "Epoch: 10/20\n",
      " Training Steps: 2944...\n",
      " Training loss: 4.0118\n",
      " 0.7591 sec/batch\n",
      "\n",
      "Epoch: 10/20\n",
      " Training Steps: 3008...\n",
      " Training loss: 3.9333\n",
      " 0.7635 sec/batch\n",
      "\n",
      "Epoch: 10/20\n",
      " Training Steps: 3072...\n",
      " Training loss: 3.9563\n",
      " 0.7603 sec/batch\n",
      "\n",
      "Epoch: 11/20\n",
      " Training Steps: 3136...\n",
      " Training loss: 3.8933\n",
      " 0.7597 sec/batch\n",
      "\n",
      "Epoch: 11/20\n",
      " Training Steps: 3200...\n",
      " Training loss: 3.9114\n",
      " 0.7592 sec/batch\n",
      "\n",
      "Epoch: 11/20\n",
      " Training Steps: 3264...\n",
      " Training loss: 3.7825\n",
      " 0.7601 sec/batch\n",
      "\n",
      "Epoch: 11/20\n",
      " Training Steps: 3328...\n",
      " Training loss: 3.8866\n",
      " 0.7603 sec/batch\n",
      "\n",
      "Epoch: 11/20\n",
      " Training Steps: 3392...\n",
      " Training loss: 3.8415\n",
      " 0.7587 sec/batch\n",
      "\n",
      "Epoch: 12/20\n",
      " Training Steps: 3456...\n",
      " Training loss: 3.8425\n",
      " 0.7608 sec/batch\n",
      "\n",
      "Epoch: 12/20\n",
      " Training Steps: 3520...\n",
      " Training loss: 3.9030\n",
      " 0.7611 sec/batch\n",
      "\n",
      "Epoch: 12/20\n",
      " Training Steps: 3584...\n",
      " Training loss: 3.9242\n",
      " 0.7582 sec/batch\n",
      "\n",
      "Epoch: 12/20\n",
      " Training Steps: 3648...\n",
      " Training loss: 3.7349\n",
      " 0.7612 sec/batch\n",
      "\n",
      "Epoch: 13/20\n",
      " Training Steps: 3712...\n",
      " Training loss: 3.7435\n",
      " 0.7600 sec/batch\n",
      "\n",
      "Epoch: 13/20\n",
      " Training Steps: 3776...\n",
      " Training loss: 3.7615\n",
      " 0.7589 sec/batch\n",
      "\n",
      "Epoch: 13/20\n",
      " Training Steps: 3840...\n",
      " Training loss: 3.7334\n",
      " 0.7591 sec/batch\n",
      "\n",
      "Epoch: 13/20\n",
      " Training Steps: 3904...\n",
      " Training loss: 3.7057\n",
      " 0.7603 sec/batch\n",
      "\n",
      "Epoch: 13/20\n",
      " Training Steps: 3968...\n",
      " Training loss: 3.6730\n",
      " 0.7596 sec/batch\n",
      "\n",
      "Epoch: 14/20\n",
      " Training Steps: 4032...\n",
      " Training loss: 3.7275\n",
      " 0.7611 sec/batch\n",
      "\n",
      "Epoch: 14/20\n",
      " Training Steps: 4096...\n",
      " Training loss: 3.7483\n",
      " 0.7636 sec/batch\n",
      "\n",
      "Epoch: 14/20\n",
      " Training Steps: 4160...\n",
      " Training loss: 3.6845\n",
      " 0.7603 sec/batch\n",
      "\n",
      "Epoch: 14/20\n",
      " Training Steps: 4224...\n",
      " Training loss: 3.7710\n",
      " 0.7620 sec/batch\n",
      "\n",
      "Epoch: 14/20\n",
      " Training Steps: 4288...\n",
      " Training loss: 3.6115\n",
      " 0.7629 sec/batch\n",
      "\n",
      "Epoch: 15/20\n",
      " Training Steps: 4352...\n",
      " Training loss: 3.5685\n",
      " 0.7615 sec/batch\n",
      "\n",
      "Epoch: 15/20\n",
      " Training Steps: 4416...\n",
      " Training loss: 3.6100\n",
      " 0.7602 sec/batch\n",
      "\n",
      "Epoch: 15/20\n",
      " Training Steps: 4480...\n",
      " Training loss: 3.6200\n",
      " 0.7681 sec/batch\n",
      "\n",
      "Epoch: 15/20\n",
      " Training Steps: 4544...\n",
      " Training loss: 3.6255\n",
      " 0.7630 sec/batch\n",
      "\n",
      "Epoch: 15/20\n",
      " Training Steps: 4608...\n",
      " Training loss: 3.5335\n",
      " 0.7618 sec/batch\n",
      "\n",
      "Epoch: 16/20\n",
      " Training Steps: 4672...\n",
      " Training loss: 3.6435\n",
      " 0.7610 sec/batch\n",
      "\n",
      "Epoch: 16/20\n",
      " Training Steps: 4736...\n",
      " Training loss: 3.5746\n",
      " 0.7641 sec/batch\n",
      "\n",
      "Epoch: 16/20\n",
      " Training Steps: 4800...\n",
      " Training loss: 3.5551\n",
      " 0.7600 sec/batch\n",
      "\n",
      "Epoch: 16/20\n",
      " Training Steps: 4864...\n",
      " Training loss: 3.5936\n",
      " 0.7596 sec/batch\n",
      "\n",
      "Epoch: 16/20\n",
      " Training Steps: 4928...\n",
      " Training loss: 3.4929\n",
      " 0.7631 sec/batch\n",
      "\n",
      "Epoch: 17/20\n",
      " Training Steps: 4992...\n",
      " Training loss: 3.5087\n",
      " 0.7616 sec/batch\n",
      "\n",
      "Epoch: 17/20\n",
      " Training Steps: 5056...\n",
      " Training loss: 3.6038\n",
      " 0.7593 sec/batch\n",
      "\n",
      "Epoch: 17/20\n",
      " Training Steps: 5120...\n",
      " Training loss: 3.4616\n",
      " 0.7617 sec/batch\n",
      "\n",
      "Epoch: 17/20\n",
      " Training Steps: 5184...\n",
      " Training loss: 3.4803\n",
      " 0.7660 sec/batch\n",
      "\n",
      "Epoch: 17/20\n",
      " Training Steps: 5248...\n",
      " Training loss: 3.5172\n",
      " 0.7623 sec/batch\n",
      "\n",
      "Epoch: 18/20\n",
      " Training Steps: 5312...\n",
      " Training loss: 3.4653\n",
      " 0.7617 sec/batch\n",
      "\n",
      "Epoch: 18/20\n",
      " Training Steps: 5376...\n",
      " Training loss: 3.4317\n",
      " 0.7642 sec/batch\n",
      "\n",
      "Epoch: 18/20\n",
      " Training Steps: 5440...\n",
      " Training loss: 3.4591\n",
      " 0.7618 sec/batch\n",
      "\n",
      "Epoch: 18/20\n",
      " Training Steps: 5504...\n",
      " Training loss: 3.4425\n",
      " 0.7639 sec/batch\n",
      "\n",
      "Epoch: 19/20\n",
      " Training Steps: 5568...\n",
      " Training loss: 3.3965\n",
      " 0.7622 sec/batch\n",
      "\n",
      "Epoch: 19/20\n",
      " Training Steps: 5632...\n",
      " Training loss: 3.3636\n",
      " 0.7629 sec/batch\n",
      "\n",
      "Epoch: 19/20\n",
      " Training Steps: 5696...\n",
      " Training loss: 3.4285\n",
      " 0.7604 sec/batch\n",
      "\n",
      "Epoch: 19/20\n",
      " Training Steps: 5760...\n",
      " Training loss: 3.5140\n",
      " 0.7697 sec/batch\n",
      "\n",
      "Epoch: 19/20\n",
      " Training Steps: 5824...\n",
      " Training loss: 3.3458\n",
      " 0.7623 sec/batch\n",
      "\n",
      "Epoch: 20/20\n",
      " Training Steps: 5888...\n",
      " Training loss: 3.3966\n",
      " 0.7642 sec/batch\n",
      "\n",
      "Epoch: 20/20\n",
      " Training Steps: 5952...\n",
      " Training loss: 3.3673\n",
      " 0.7616 sec/batch\n",
      "\n",
      "Epoch: 20/20\n",
      " Training Steps: 6016...\n",
      " Training loss: 3.3640\n",
      " 0.7619 sec/batch\n",
      "\n",
      "Epoch: 20/20\n",
      " Training Steps: 6080...\n",
      " Training loss: 3.3540\n",
      " 0.7606 sec/batch\n",
      "\n",
      "Epoch: 20/20\n",
      " Training Steps: 6144...\n",
      " Training loss: 3.3709\n",
      " 0.7639 sec/batch\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    counter = 0\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        \n",
    "        for x, y in next_batch(encoded, sequence_size, steps):\n",
    "            counter += 1\n",
    "            start = time.time()\n",
    "            feed = {model.x_input: x, model.y_target: y, model.keep_prob: keep_prob, \n",
    "                   model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.loss, model.final_state, model.optimizer], feed_dict = feed)\n",
    "            \n",
    "            if counter % 64 == 0:\n",
    "                end = time.time()\n",
    "                print('Epoch: {}/{}\\n'.format(e+1, epochs), \n",
    "                     'Training Steps: {}...\\n'.format(counter),\n",
    "                     'Training loss: {:.4f}\\n'.format(batch_loss),\n",
    "                     '{:.4f} sec/batch\\n'.format(end-start))\n",
    "            \n",
    "            if counter % 1000 == 0:\n",
    "                saver.save(sess, 'Songci_checkpoint/i{}.ckpt'.format(counter))\n",
    "                \n",
    "    saver.save(sess, 'Songci_checkpoint/i{}.ckpt'.format(counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size , top_n = 15):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p = p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_size = 64\n",
    "steps = 50\n",
    "num_units = 512\n",
    "num_layers = 4\n",
    "learning_rate = 0.0001\n",
    "keep_prob = 0.95\n",
    "num_classes = len(word_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(checkpoints, n_samples, num_units, vocab_size, prime = '我'):\n",
    "    samples = [c for c in prime]\n",
    "    model = CharRNN(learning_rate = learning_rate, num_classes=num_classes, steps = steps, num_layers = num_layers,\n",
    "                    num_units = 512, sequence_size = sequence_size, sampling = True)\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoints)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        \n",
    "        for c in prime:\n",
    "            x = np.zeros((1,1))\n",
    "            x[0,0] = word_to_int[c]\n",
    "            feed = {model.x_input: x, model.keep_prob: 1.0, model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], feed_dict = feed)\n",
    "        \n",
    "        c = pick_top_n(preds, len(word_to_int))\n",
    "        samples.append(int_to_word[c])\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            x[0, 0] = c\n",
    "            feed = {model.x_input: x, model.keep_prob: 1.0, model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], feed_dict = feed)\n",
    "            \n",
    "            c = pick_top_n(preds, len(word_to_int))\n",
    "            samples.append(int_to_word[c])\n",
    "    \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Songci_checkpoint/i6180.ckpt'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint('Songci_checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from Songci_checkpoint/i6180.ckpt\n",
      "\n",
      "黄鹂。今年春事，早向一帘残雪。待著春时多意了，又管得、桃花红萼。\n",
      "嫩红初试胭脂透。玉妃下、玉容侧。玉人端向花钿薄。念小雨、低斜细细。好夜逐、东郊更暖。\n",
      "天外莺声，燕飞花下梅花瘦。卷帘成影。几点斜阳雨。一日归时，只解穿阑柳。休相恋。乱山千嶂。欲向人千里。\n",
      "绿叶红萸，满城芳草花中树。碧纱窗外。燕燕双声起。一点清霜，恼得江南去。知何处。海棠开了。总是花间主。\n",
      "春色如何，不胜老来风景。为谁怜我。一片闲云暖。一掬清凉，一叶清明月。还相恋。乱山何处。明月花开遍。\n",
      "画阁帘栊，月淡疏疏，更霜渐过。向晓寒疏影，香风冉冉，人间梦渡，还听单丝。梦里春风，又教人去，独倚阑干春水西。沈沈久，望长安市上，月影回文。愁肠无限奚人。似小院朱帘十二愁。恨流莺入曲，青芜空远，东风又暖，花影初晴。最好重阳，花枝月下，约略不堪吹到行。还休道，这一回一点，一片相思。\n",
      "五百三宫，花信争妍，花前似梅。看紫纱宫殿，云收锦阁，冰绡袖稳，绿野红堆。玉瑟重腰，鹅花捣润，件曲柔人尤可怜。空思处，又残蝉更过，却是愁愁。离愁。一点尘埃。况此去人生第一回。况暮云无赖，残烟冉冉，云横雁翅，落日云收。梦入楼涯，梦归还是，千里楼台烟浪声。长沙路，望天涯雁远，\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'Songci_checkpoint/i6180.ckpt'\n",
    "samp = sample(checkpoint, 500, 2048, len(word_to_int), prime='黄鹂')\n",
    "print('')\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
